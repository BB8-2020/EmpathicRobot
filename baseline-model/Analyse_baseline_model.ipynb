{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse baseline model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document we will create a baseline model that is the simplest solution for us.\n",
    "This baseline model was created with a CNN network. The structure of the model can be found in [this](https://github.com/BB8-2020/EmpathicRobot/blob/baseline-model/baseline-model/baseline_model.py) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model python code \n",
    "import baseline_model as ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us start with creating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = ml.create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary looks as here below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "__1. Conv Layer__\n",
    "\n",
    "The first layer consists of 32 3x3 filters with ReLU. We set the input of this layer equal to the shape of the train data, which is (48, 48, 1).\n",
    "\n",
    "We leave the stride and padding at the default value. In the next model we are going to play with this but for now we keep it at 1 and \"vaild\".\n",
    "\n",
    "We do add a Batch normalization. The output of this layer (the activation map) is (46, 46, 32).\n",
    "\n",
    "__2. Conv Layer__\n",
    "\n",
    "The second layer consists of 64 filters of 3x3 and here we apply batch normalization. The output of this layer woudld be (44, 44, 64) \n",
    "\n",
    "__3. Conv Layer__\n",
    "\n",
    "The last conv layer consists of 128 filters of 3x3 and we also apply a max pooling of (2,2) that produce output shape of (21, 21, 128). We also add a batch normalization to this layer. \n",
    "\n",
    "To this layer we add a flatten option, that means the output shape of this layer would be (56448) \n",
    "\n",
    "__4. fully connected layer__\n",
    "\n",
    "This layer takes (56448) as input. The output shape of this layer is the probability of 2 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, time to compile!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compile and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the model we use Adam optimaizer and categorical_crossentropy as los function. Let us now train the model. We start by reading the correct json dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter is the correct path to the json file.\n",
    "frame = ml.read_data(\"../../BB8/ferPlus_data_json/train_happy_frame.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the trainsets and use these to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trainsets are created when calling the train function.\n",
    "history = ml.train_model(baseline_model, frame, 256, 10, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier beschrijf je het resultaat van het model, teken hierbij een mooi plot van de acc en loss fucntie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deze functie mag je ergens anders platsen + docuemteren \n",
    "def plot_results(history):\n",
    "    \"\"\"Plot the results of the fit function of the model.\n",
    "       takes history: A nummpy array as input.\"\"\"\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier zeg we wat de resultaten van het model betekennen en waarvoor we deze resultaten precies gaan gebruiken. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
