{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse and run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to build, test, analyze and compare the model with the previous versions. This is followed by improvements to the model and the data. We run this cycle a few times until we achieve realistic and nice results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's start by importing the necessary libraries and files, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the data \n",
    "import pickle\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import bz2\n",
    "\n",
    "# Import the file with the model functions\n",
    "import sys\n",
    "sys.path.append('/Users/marya/PycharmProjects/EmpathicRobot')\n",
    "from conv_model import *\n",
    "from models.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the models can still be found [here](https://github.com/BB8-2020/EmpathicRobot/blob/classification-model/models/classification-model/conv/conv_model.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FerPlus__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done before, our data is ready to use. In this section we will first use **ferPlus** to train the model. After that we would us the **AffectNet** to train the model. This data has already been read, prepared and stored in **hier linkje zetten** this file. \n",
    "For now, our data is in a pickel file that we will read as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we set up the path to the data as follows, you can also set it to your own path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.getcwd() + '/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately split the data into train, test and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainf, y_trainf, x_valf, y_valf, x_testf, y_testf = cPickle.load(bz2.BZ2File('ferPlus_processed', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the data consists of train set that contains 80% of the data. The validation and the test set are equal in size 20% and are used to subsequently test the model.\n",
    "\n",
    "This data has already been cleaned and normalized so we don't have to do anything with the data anymore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set:\\nX_train shape:{x_trainf.shape}\\nY_train shape:{y_trainf.shape}\\n\")\n",
    "\n",
    "print(f\"Test set:\\nX_test shape:{x_testf.shape}\\nY_test shape:{y_testf.shape}\\n\")\n",
    "\n",
    "print(f\"Validation set:\\nX_val shape:{x_valf.shape}\\nY_val shape:{y_valf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__AffectNet__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the FerPlus. So we split the data first and print the shape out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traina, y_traina, x_vala, y_vala, x_testa, y_testa = cPickle.load(bz2.BZ2File('affectNet_processed', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set:\\nX_train shape:{x_trainf.shape}\\nY_train shape:{y_trainf.shape}\\n\")\n",
    "\n",
    "print(f\"Test set:\\nX_test shape:{x_testf.shape}\\nY_test shape:{y_testf.shape}\\n\")\n",
    "\n",
    "print(f\"Validation set:\\nX_val shape:{x_valf.shape}\\nY_val shape:{y_valf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is finally time to start working on the model.\n",
    "We are going to start with the following model:\n",
    "\n",
    "### Creat model\n",
    "\n",
    "__1. Conv Layer__\n",
    "\n",
    "The first layer consists of 64 3x3 filters with ReLU. We set the input of this layer equal to the shape of the train data, which is (48, 48, 1).\n",
    "We leave the stride and padding at the default value. We do add a Batch normalization. The output of this layer (the activation \n",
    "map) is (46, 46, 64).\n",
    "As laste we add a dropout of 0.5. That brings us to a result of (46, 46, 64)\n",
    "\n",
    "__2. Conv Layer__\n",
    "\n",
    "\n",
    "The second layer consists of 64 filters of 3x3 and here we apply relu as well. The output of this layer woudld be (44, 44, 64). we also apply a max pooling of (2,2) and strides od (2,2) that produces an output shape of (44, 44, 64).\n",
    "\n",
    "__3. Conv Layer__\n",
    "\n",
    "\n",
    "This layer consists of 128 filter of 3x3 we also apply relu here. The output of this layer would be (20, 20, 128). We do add  Batch normalization. The output of this layer is (20, 20, 128).\n",
    "\n",
    "__4. Conv Layer__\n",
    "\n",
    "\n",
    "This layer consists of 128 filter of 3x3 we also apply relu here. The output of this layer would be (18, 18, 128). We also apply a max pooling of (2,2) and strides od (2,2) that produces an output shape of (9, 9, 128).\n",
    "\n",
    "__5. Conv Layer__ \n",
    "\n",
    "\n",
    "The last layer consists of 256 fliter of 3x3 and we apply reule. The output would be (7, 7, 256). We apply here also a max pooling of (2,2) and strides od (2,2) that produces an output shape of (7, 7, 256).\n",
    "To this layer we add a flatten option, that means the output shape of this layer would be (12544)\n",
    "\n",
    "__3. Fully connected layer__\n",
    "\n",
    "\n",
    "This layer takes (12544) as input. We apply a droupout of 0.2. The next dense layer teaks (1024) as input. After applying the drouput for the last time the output shape of this layer is the probability of 7 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create all the models that we got \n",
    "models = build_models(input_shape=(48, 48, 1), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sequential model version 1\n",
    "model1 = Sequential(models[0]['layers'], name = models[0]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the summary out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, time to compile!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compile and train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the model we use Adam optimaizer and binary crossentropy as los function. Let us now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the epochs to 100 and the batch size tot 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Train with ferPlus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fer = model1.fit(x_trainf, y_trainf, batch_size=batch_size, epochs=epochs,\n",
    "                        steps_per_epoch=len(x_trainf) // batch_size,\n",
    "                        validation_data=(x_valf, y_valf), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to test our model using the test set for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_fer, test_acc_fer = model1.evaluate(x_testf, y_testf, batch_size=batch_size)\n",
    "print(f\"Test loss: {test_loss_fer:.4f}\")\n",
    "print(f\"Test accuracy: {test_loss_fer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bespreek de resultaten van deze train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Train with AffectNet_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, the results are not too great. Therefore we will now try to adjust the settings of the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_affect = model1.fit(x_traina, y_traina, batch_size=batch_size, epochs=epochs,\n",
    "                        steps_per_epoch=len(x_traina) // batch_size,\n",
    "                        validation_data=(x_vala, y_vala), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_affect, test_acc_affect = model1.evaluate(x_testa, y_testa, batch_size=batch_size)\n",
    "print(f\"Test loss: {test_loss_affect:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc_affect:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bespreek de resultaten van deze train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "Let's discuss the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(history_fer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(history_affect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: BESPREKK DE RESULTATEN!!!! MAAR EERST MOET HET MODEL GETRAIEND WORDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, fix wat hieronder staat!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "#### Create model\n",
    "\n",
    "__1. Conv Layer__\n",
    "\n",
    "The first layer consists of 64 3x3 filters with ReLU. We set the input of this layer equal to the shape of the train data, which is (48, 48, 1).\n",
    "We leave the stride and padding at the default value. We do add a Batch normalization. The output of this layer (the activation \n",
    "map) is (46, 46, 64).\n",
    "As laste we add a dropout of 0.5. That brings us to a result of (46, 46, 64)\n",
    "\n",
    "__2. Conv Layer__\n",
    "\n",
    "\n",
    "The second layer consists of 64 filters of 3x3 and here we apply relu as well. The output of this layer woudld be (44, 44, 64). we also apply a max pooling of (2,2) and strides od (2,2) that produces an output shape of (44, 44, 64).\n",
    "\n",
    "__3. Conv Layer__\n",
    "\n",
    "\n",
    "This layer consists of 128 filter of 3x3 we also apply relu here. The output of this layer would be (20, 20, 128). We do add  Batch normalization. The output of this layer is (20, 20, 128).\n",
    "\n",
    "__4. Conv Layer__\n",
    "\n",
    "\n",
    "This layer consists of 128 filter of 3x3 we also apply relu here. The output of this layer would be (18, 18, 128). We also apply a max pooling of (2,2) and strides od (2,2) that produces an output shape of (9, 9, 128).\n",
    "\n",
    "__5. Conv Layer__ \n",
    "\n",
    "\n",
    "The last layer consists of 256 fliter of 3x3 and we apply reule. The output would be (7, 7, 256). We apply here also a max pooling of (2,2) and strides od (2,2) that produces an output shape of (7, 7, 256).\n",
    "To this layer we add a flatten option, that means the output shape of this layer would be (12544)\n",
    "\n",
    "__3. Fully connected layer__\n",
    "\n",
    "\n",
    "This layer takes (12544) as input. We apply a droupout of 0.2. The next dense layer teaks (1024) as input. After applying the drouput for the last time the output shape of this layer is the probability of 7 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential(models[1]['layers'], name = models[1]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compile and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Train with ferPlus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fer = model2.fit(x_trainf, y_trainf, batch_size=batch_size, epochs=epochs,\n",
    "                        steps_per_epoch=len(x_trainf) // batch_size,\n",
    "                        validation_data=(x_valf, y_valf), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_fer, test_acc_fer = model1.evaluate(x_testf, y_testf, batch_size=batch_size)\n",
    "print(f\"Test loss: {test_loss_fer:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc_fer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Train with AffectNet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_affect = model2.fit(x_traina, y_traina, batch_size=batch_size, epochs=epochs,\n",
    "                        steps_per_epoch=len(x_traina) // batch_size,\n",
    "                        validation_data=(x_vala, y_vala), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_affect, test_acc_affect = model1.evaluate(x_testa, y_testa, batch_size=batch_size)\n",
    "print(f\"Test loss: {test_loss_affect:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc_affect:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(history_fer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(history_affect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: BESPREKK DE RESULTATEN!!!! MAAR EERST MOET HET MODEL GETRAIEND WORDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are going beter, next we are going to try to add some argumentation to the data. That could help our model to leren more. You can find the file where the data has been argumendated right [hier](linkjeee) __fix it!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_FerPlus_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen,x_train_arg, y_train_arg, x_val_arg, y_val_arg, x_test_arg, y_test_arg =\n",
    "                                                                            cPickle.load(bz2.BZ2File('ferPlus_augment', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to just fit the model using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fer = model2.fit(datagen.flow(x_train_arg, y_train_arg, batch_size=batch_size), epochs=epochs,\n",
    "                            steps_per_epoch=len(x_train_arg) // batch_size,\n",
    "                            validation_data=(x_val_arg, y_val_arg), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_fer, test_acc_fer = model2.evaluate(x_test_arg, y_test_arg, batch_size=batch_size)\n",
    "print(f\"Test loss: {test_loss_fer:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc_fer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_AffectNet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen,x_train_arg, y_train_arg, x_val_arg, y_val_arg, x_test_arg, y_test_arg =\n",
    "                                                                            cPickle.load(bz2.BZ2File('affectNet_augment', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_affect = model2.fit(datagen.flow(x_train_arg, y_train_arg, batch_size=batch_size), epochs=epochs,\n",
    "                            steps_per_epoch=len(x_train_arg) // batch_size,\n",
    "                            validation_data=(x_val_arg, y_val_arg), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_affect, test_acc_affect = model2.evaluate(x_test_arg, y_test_arg, batch_size=batch_size)\n",
    "print(f\"Test loss: {test_loss_affect:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc_affect:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(history_fer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(history_fer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: BESPREKK DE RESULTATEN!!!! MAAR EERST MOET HET MODEL GETRAIEND WORDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
